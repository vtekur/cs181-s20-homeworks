{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjDqx4D4HEZm"
   },
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVD8gFOuGuVR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# might need to install if not using colab\n",
    "import torch\n",
    "\n",
    "# parameters\n",
    "N = 2000\n",
    "M = 100\n",
    "H = 75\n",
    "\n",
    "# generate data\n",
    "np.random.seed(181)\n",
    "W1 = np.random.normal(scale=0.1, size=(H, M))\n",
    "b1 = np.random.normal(scale=0.1, size=H)\n",
    "W2 = np.random.normal(scale=0.1, size=H)\n",
    "b2 = np.random.normal(scale=0.1, size=1)\n",
    "\n",
    "X = np.random.random((N, M))\n",
    "y = np.random.randint(0,2,size=N).astype('float')\n",
    "\n",
    "# torch copies of data\n",
    "tW1 = torch.tensor(W1, requires_grad=True)\n",
    "tb1 = torch.tensor(b1, requires_grad=True)\n",
    "tW2 = torch.tensor(W2, requires_grad=True)\n",
    "tb2 = torch.tensor(b2, requires_grad=True)\n",
    "\n",
    "tX = torch.tensor(X)\n",
    "ty = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlIeAcUyG15H"
   },
   "outputs": [],
   "source": [
    "# CAREFUL: if you run this cell multiple times w/o running the above cell,\n",
    "# the gradients will accumulate in the grad variables. Rerun the above cell\n",
    "# to reset\n",
    "\n",
    "# torch implementation\n",
    "def tforward(X):\n",
    "  z1 = (torch.mm(tX, tW1.T) + tb1).sigmoid()\n",
    "  X = (torch.mv(z1, tW2) + tb2).sigmoid()\n",
    "  return X\n",
    "\n",
    "tyhat = tforward(tX)\n",
    "L = -((ty * tyhat.log()) + (1-ty) * (1 - tyhat).log())\n",
    "# the magic of autograd!\n",
    "L.sum().backward()\n",
    "\n",
    "# the gradients will be stored in the following variables\n",
    "grads_truth = [tW1.grad.numpy(), tb1.grad.numpy(), tW2.grad.numpy(), tb2.grad.numpy()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKcMeplRG34R"
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# use this to check your implementation\n",
    "# you can pass in grads_truth as truth and the output of get_grads as our_impl\n",
    "def compare_grads(truth, our_impl):\n",
    "  for elt1, elt2 in zip(truth, our_impl):\n",
    "    if not np.allclose(elt1, elt2, atol=0.001, rtol=0.001):\n",
    "      return False\n",
    "  \n",
    "  return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIQBOv7AWR9Z"
   },
   "outputs": [],
   "source": [
    "# Implement the forward pass of the data. Perhaps you can return some variables that \n",
    "# will be useful for calculating the gradients. \n",
    "def forward(X):\n",
    "  return X\n",
    "\n",
    "# Code the gradients you found in part 2.\n",
    "# Can pass in additional arguments\n",
    "def get_grads(y, yhat, X): \n",
    "  dLdb2 = None\n",
    "  dLdW2 = None\n",
    "  dLdb1 = None\n",
    "  dLdW1 = None\n",
    "  \n",
    "  # make sure this order is kept for the compare function\n",
    "  return [dLdW1, dLdb1, dLdW2, dLdb2]\n",
    "\n",
    "# don't need to change this, but can if you want"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "T3Skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
